{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, roc_auc_score, mean_absolute_error, confusion_matrix, classification_report \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(txt_file, y_true, y_pred, y_prob):\n",
    "    print('Model Metrics: \\n')\n",
    "    txt_file.write('Model Metrics: \\n')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cr = classification_report(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    txt_file.write(f'Accuracy: {accuracy}\\n')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    txt_file.write(f'F1 Score: {f1}\\n')\n",
    "    print(f'Cohen Kappa Score: {kappa}')\n",
    "    txt_file.write(f'Cohen Kappa Score: {kappa}\\n')\n",
    "    print(f'Confusion Matrix:\\n {cm}')\n",
    "    txt_file.write(f'Confusion Matrix:\\n {cm}\\n')\n",
    "    print(f'Classification Report:\\n {cr}')\n",
    "    txt_file.write(f'Classification Report:\\n {cr}\\n')\n",
    "    \n",
    "    print(f'MAE : {mae}')\n",
    "    txt_file.write(f'MAE : {mae}\\n')\n",
    "    print(f'ROC AUC Score: {roc_auc}')\n",
    "    txt_file.write(f'ROC AUC Score: {roc_auc}\\n')\n",
    "    \n",
    "    return {'accuracy': accuracy, 'f1': f1, 'kappa': kappa, 'mae': mae, 'roc_auc': roc_auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(save_path, metric_data, metric_list):\n",
    "    df_metrics = pd.DataFrame(metric_data).T\n",
    "    df_metrics_melted = df_metrics[metric_list].reset_index().melt(id_vars='index')\n",
    "    df_metrics_melted.rename(columns={'index': 'Model', 'variable': 'Metric', 'value': 'Score'}, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Metric', y='Score', hue='Model', data=df_metrics_melted)\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, column, lower_threshold=None, upper_threshold=None):\n",
    "    print(f'{column} - Size before removing outliers: {df.shape}')\n",
    "    if lower_threshold:\n",
    "        df = df[df[column] >= lower_threshold]\n",
    "    if upper_threshold:\n",
    "        df = df[df[column] <= upper_threshold]\n",
    "    print(f'{column} -Size after removing outliers: {df.shape}')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    df = remove_outliers(df, \"ap_hi\", 50, 160)\n",
    "    df = remove_outliers(df, \"ap_lo\", 20, 110)\n",
    "    \n",
    "    df = df.drop(columns=[\"id\"]) \n",
    "    df[\"BMI\"] = df[\"weight\"] / (df[\"height\"] / 100) ** 2\n",
    "    df['age'] = df['age'].apply(lambda x: x / 365.25).astype('int16')\n",
    "    df = df.drop(columns=[\"weight\", \"height\"])\n",
    "    \n",
    "    df = remove_outliers(df, \"BMI\", 14, 50)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    result_path = './results'\n",
    "    now = datetime.now()\n",
    "    folder_name = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    os.makedirs(os.path.join(result_path, folder_name), exist_ok=True)\n",
    "    print('Results will be saved in: ', folder_name)\n",
    "    txt_file = open(os.path.join(result_path, folder_name, 'results.txt'), 'a')\n",
    "    txt_file.write('Multiple Model\\n')\n",
    "\n",
    "    data = pd.read_csv('cardio_train.csv', sep=';')\n",
    "    \n",
    "    transformed_data = preprocess_data(data)\n",
    "    \n",
    "    X = transformed_data.drop('cardio', axis=1)\n",
    "    y = transformed_data['cardio']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    models = {\n",
    "        'randomForest': RandomForestClassifier(random_state=42),\n",
    "        'gradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "        'catBoost': CatBoostClassifier(random_state=42, verbose=0)\n",
    "    }\n",
    "\n",
    "    param_grids = {\n",
    "        'randomForest': {\n",
    "            'n_estimators': [100, 300, 500],\n",
    "            'max_depth': [None, 50, 100],\n",
    "            'criterion': ['gini', 'entropy']\n",
    "        },\n",
    "        'gradientBoosting': {\n",
    "            'learning_rate': [0.1, 0.01, 0.001],\n",
    "            'n_estimators': [100, 300, 500],\n",
    "            'max_depth': [3, 5, 10]\n",
    "        },\n",
    "        'catBoost': {\n",
    "            'depth': [3, 5, 7],\n",
    "            'learning_rate': [0.1, 0.01, 0.001],\n",
    "            'iterations': [100, 300, 500]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metric_data = {}\n",
    "\n",
    "    print(\"Models Configured\")\n",
    "    for model_name, model in models.items():\n",
    "        grid = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=kf, scoring='f1_macro')\n",
    "        pipe = Pipeline([\n",
    "            ('scaler', StandardScaler(random_state=42)),\n",
    "            ('grid', grid)\n",
    "        ])\n",
    "        print(datetime.now())\n",
    "        print(\"Training started\")\n",
    "        pipe.fit(X_train, y_train)\n",
    "        print(datetime.now())\n",
    "        print(f'\\n\\nModel: {model_name}')\n",
    "        txt_file.write(f'\\n\\nModel: {model_name}\\n')\n",
    "        print(f'Best params: {pipe.named_steps[\"grid\"].best_params_}')\n",
    "        txt_file.write(f'Best params: {pipe.named_steps[\"grid\"].best_params_}\\n')\n",
    "        print(f'Best validation score: {pipe.named_steps[\"grid\"].best_score_}')\n",
    "        txt_file.write(f'Best validation score: {pipe.named_steps[\"grid\"].best_score_}\\n')\n",
    "\n",
    "        best_model = pipe.named_steps['grid'].best_estimator_\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        model_save_path = os.path.join(result_path, folder_name, f'{model_name}.joblib')\n",
    "        joblib.dump(best_model, model_save_path)\n",
    "        print(f'Best model saved at: {model_save_path}')\n",
    "        txt_file.write(f'Best model saved at: {model_save_path}\\n')\n",
    "        \n",
    "        scaler_save_path = os.path.join(result_path, folder_name, f'{model_name}_scaler.joblib')\n",
    "        joblib.dump(pipe.named_steps['scaler'], scaler_save_path)\n",
    "        print(f'Scaler saved at: {scaler_save_path}')\n",
    "        txt_file.write(f'Scaler saved at: {scaler_save_path}\\n')\n",
    "\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "        model_metrics = metrics(txt_file, y_test, y_test_pred, y_prob)\n",
    "        \n",
    "        metric_data[model_name] = model_metrics\n",
    "        \n",
    "          \n",
    "    txt_file.close()\n",
    "    plot_metrics(os.path.join(result_path, folder_name, 'model_comparison.png'), metric_data, ['accuracy', 'f1', 'mae', 'roc_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now())\n",
    "model()\n",
    "print(datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
